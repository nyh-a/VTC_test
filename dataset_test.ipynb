{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2120230655/.conda/envs/niu_v1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_loader.dataset import *\n",
    "from data_loader.data_loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pickled dataset\n",
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "taxonomy_name = \"semeval_food\"\n",
    "data_path = \"data/SemEval-Food/semeval_food.pickle.bin\"\n",
    "raw_graph_dataset = MAGDataset(name=taxonomy_name, path=data_path, raw=False, existing_partition=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pseudo leaf\n",
      "building node2pos, node2edge\n",
      "building valid and test node list\n",
      "924 1190\n",
      "Finish loading dataset (2.7509231567382812 seconds)\n",
      "Loading metadata from data/imgs/semeval_food_dataset_final.jsonl...\n",
      "Loaded 1488 metadata entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening dataset into a 'big pool': 100%|██████████| 1190/1190 [00:00<00:00, 24639.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11845 total training pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img_path = \"data/imgs\"\n",
    "img_feat = \"data/img_feat/semeval_food_blip_feat\"\n",
    "json_file = taxonomy_name + \"_dataset_final.jsonl\"\n",
    "json_data_path = os.path.join(img_path, json_file)\n",
    "tokenizer_path = \"/home/u2120230655/codes/VTC/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "negative_size = 4\n",
    "num_tokens = 4\n",
    "\n",
    "dataset = Dataset_Stage2(\n",
    "    graph_dataset=raw_graph_dataset,\n",
    "    json_data_path=json_data_path,\n",
    "    img_feature_dir=img_feat,\n",
    "    negative_size=negative_size,\n",
    "    tokenizer=tokenizer,\n",
    "    num_tokens=num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vis_q', 'q_seg_start', 'q_seg_end', 'vis_p', 'vis_c', 'vis_s', 'c_seg_p', 'c_seg_c', 'c_seg_s', 'c_seg_end', 'label'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset_output(dataset):\n",
    "    print(\"\\n=== Verifying Dataset Output (Single Sample) ===\")\n",
    "    \n",
    "    # 取第 0 个样本\n",
    "    item = dataset[0]\n",
    "    if item is None:\n",
    "        print(\"Error: Item 0 is None!\")\n",
    "        return\n",
    "\n",
    "    print(\"Keys present:\", item.keys())\n",
    "    \n",
    "    # 1. 检查形状\n",
    "    for k, v in item.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print(f\"{k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "        else:\n",
    "            print(f\"{k}: {type(v)}\")\n",
    "\n",
    "    # 2. 解码文本 (这是最重要的!)\n",
    "    tokenizer = dataset.tokenizer\n",
    "    \n",
    "    print(\"\\n--- Decoding Query ---\")\n",
    "    # Q: SegStart + [IMG] + SegEnd\n",
    "    # 我们把 start 和 end 拼起来看看 (虽然中间缺了 Image Token)\n",
    "    q_ids = torch.cat([item['q_seg_start'], item['q_seg_end']]).tolist()\n",
    "    print(tokenizer.decode(q_ids))\n",
    "    # 预期: [CLS] Query Node: Definition: \"...\", Image: [SEP]\n",
    "\n",
    "    print(\"\\n--- Decoding Candidate ---\")\n",
    "    # C: SegP + [IMG] + SegC + [IMG] + SegS + [IMG] + SegEnd\n",
    "    c_ids = torch.cat([\n",
    "        item['c_seg_p'], \n",
    "        torch.tensor([tokenizer.mask_token_id]), # 模拟 P_Img\n",
    "        item['c_seg_c'], \n",
    "        torch.tensor([tokenizer.mask_token_id]), # 模拟 C_Img\n",
    "        item['c_seg_s'], \n",
    "        torch.tensor([tokenizer.mask_token_id]), # 模拟 S_Img\n",
    "        item['c_seg_end']\n",
    "    ]).tolist()\n",
    "    print(tokenizer.decode(c_ids))\n",
    "    # 预期: [CLS] Parent... Img: [MASK]; Child... Img: [MASK]; Sibling... Img: [MASK]; [SEP]\n",
    "\n",
    "def verify_dataloader_output(dataloader):\n",
    "    print(\"\\n=== Verifying DataLoader Output (Batch) ===\")\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        print(\"Batch Keys:\", batch.keys())\n",
    "        \n",
    "        # 1. 检查 Tensor 维度 (Batch Size)\n",
    "        bs = batch['vis_q'].size(0)\n",
    "        print(f\"Batch Size: {bs}\")\n",
    "        \n",
    "        # 2. 检查 Image Features\n",
    "        print(f\"Visual Q shape: {batch['vis_q'].shape} (Expected: [B, 256])\")\n",
    "        \n",
    "        # 3. 检查 Mask 逻辑\n",
    "        # 打印第一个样本的 mask sum，看看长度对不对\n",
    "        for k in ['q_seg_start', 'c_seg_p']:\n",
    "            mask = batch[f\"{k}_mask\"]\n",
    "            length = mask[0].sum().item()\n",
    "            print(f\"{k} valid length (sample 0): {length}\")\n",
    "            \n",
    "        # 4. 检查 Label\n",
    "        print(f\"Labels: {batch['label'][:10]} ...\")\n",
    "        \n",
    "        break # 只看一个 Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verifying Dataset Output (Single Sample) ===\n",
      "Keys present: dict_keys(['vis_q', 'q_seg_start', 'q_seg_end', 'vis_p', 'vis_c', 'vis_s', 'c_seg_p', 'c_seg_c', 'c_seg_s', 'c_seg_end', 'label'])\n",
      "vis_q: shape=torch.Size([256]), dtype=torch.float32\n",
      "q_seg_start: shape=torch.Size([28]), dtype=torch.int64\n",
      "q_seg_end: shape=torch.Size([1]), dtype=torch.int64\n",
      "vis_p: shape=torch.Size([256]), dtype=torch.float32\n",
      "vis_c: shape=torch.Size([256]), dtype=torch.float32\n",
      "vis_s: shape=torch.Size([256]), dtype=torch.float32\n",
      "c_seg_p: shape=torch.Size([26]), dtype=torch.int64\n",
      "c_seg_c: shape=torch.Size([11]), dtype=torch.int64\n",
      "c_seg_s: shape=torch.Size([30]), dtype=torch.int64\n",
      "c_seg_end: shape=torch.Size([1]), dtype=torch.int64\n",
      "label: shape=torch.Size([]), dtype=torch.float32\n",
      "\n",
      "--- Decoding Query ---\n",
      "<s> query node : definition : \" absinth is strong green liqueur flavored with wormwood and anise \", image : </s>\n",
      "\n",
      "--- Decoding Candidate ---\n",
      "<s> parent node : definition : \" liqueur is strong highly flavored sweet liquor usually drunk after a meal \", image : <mask> ; child node : definition : \" \" ; image : <mask> ; sibling node : definition : \" maraschino is distilled from fermented juice of bitter wild marasca cherries \", image : <mask> </s>\n"
     ]
    }
   ],
   "source": [
    "verify_dataset_output(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer from /home/u2120230655/codes/VTC/all-mpnet-base-v2...\n",
      "Instantiating Dataset_Stage2...\n",
      "loading pickled dataset\n",
      "dataset loaded\n",
      "adding pseudo leaf\n",
      "building node2pos, node2edge\n",
      "building valid and test node list\n",
      "924 1190\n",
      "Finish loading dataset (2.7590415477752686 seconds)\n",
      "Loading metadata from data/imgs/semeval_food_dataset_final.jsonl...\n",
      "Loaded 1488 metadata entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening dataset into a 'big pool': 100%|██████████| 1190/1190 [00:00<00:00, 25076.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11845 total training pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = Stage2DataLoader(\n",
    "    data_path=data_path,\n",
    "    taxonomy_name=taxonomy_name,\n",
    "    img_root_dir=img_path,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    img_feat_dir=img_feat,\n",
    "    num_image_tokens=4,\n",
    "    batch_size=8,\n",
    "    negative_size=negative_size,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verifying DataLoader Output (Batch) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Keys: dict_keys(['vis_q', 'vis_p', 'vis_c', 'vis_s', 'label', 'q_seg_start_ids', 'q_seg_start_mask', 'q_seg_end_ids', 'q_seg_end_mask', 'c_seg_p_ids', 'c_seg_p_mask', 'c_seg_c_ids', 'c_seg_c_mask', 'c_seg_s_ids', 'c_seg_s_mask', 'c_seg_end_ids', 'c_seg_end_mask'])\n",
      "Batch Size: 8\n",
      "Visual Q shape: torch.Size([8, 256]) (Expected: [B, 256])\n",
      "q_seg_start valid length (sample 0): 27\n",
      "c_seg_p valid length (sample 0): 30\n",
      "Labels: tensor([0., 0., 0., 0., 0., 1., 1., 0.]) ...\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader_output(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niu_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
